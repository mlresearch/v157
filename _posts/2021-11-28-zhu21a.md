---
title: Cautious Actor-Critic
section: Contributed Papers
crossref: acml21
abstract: The oscillating performance of off-policy learning and persisting errors
  in the actor-critic(AC) setting call for algorithms that can conservatively learn
  to suit the stability-critical applications  better.   In  this  paper,  we  propose  a  novel  off-policy  AC  algorithm  cautious
  actor-critic (CAC). The name cautious comes from the doubly conservative nature
  that we exploit the classic policy interpolation from conservative policy iteration
  for the actor and the entropy-regularization of conservative value iteration for
  the critic.  Our key observation is the entropy-regularized critic facilitates and
  simplifies the unwieldy interpolated actor update while still ensuring robust policy
  improvement.  We compare CAC to state-of-the-art AC methods on a set of challenging
  continuous control problems and demonstrate thatCAC achieves comparable performance
  while significantly stabilizes learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu21a
month: 0
tex_title: Cautious Actor-Critic
firstpage: 220
lastpage: 235
page: 220-235
order: 220
cycles: false
bibtex_author: Zhu, Lingwei and Kitamura, Toshinori and Takamitsu, Matsubara
author:
- given: Lingwei
  family: Zhu
- given: Toshinori
  family: Kitamura
- given: Matsubara
  family: Takamitsu
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/zhu21a/zhu21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/zhu21a/zhu21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
