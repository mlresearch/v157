---
title: An Optimistic Acceleration of AMSGrad for Nonconvex Optimization
section: Contributed Papers
crossref: acml21
abstract: We propose a new variant of AMSGrad (Reddi et. al., 2018), a popular adaptive
  gradient based optimization algorithm widely used for training deep neural networks.
  Our algorithm adds prior knowledge about the sequence of consecutive mini-batch
  gradients and leverages its underlying structure making the gradients sequentially
  predictable. By exploiting the predictability process and ideas from optimistic
  online learning, the proposed algorithm can accelerate the convergence and increase
  its sample efficiency. After establishing a tighter upper bound under some convexity
  conditions on the regret, we offer a complimentary view of our algorithm which generalizes
  to the offline and stochastic nonconvex optimization settings. In the nonconvex
  case, we establish a non-asymptotic convergence bound independent of the initialization.
  We illustrate, via numerical experiments, the practical speedup on several deep
  learning models and benchmark datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21c
month: 0
tex_title: An Optimistic Acceleration of AMSGrad for Nonconvex Optimization
firstpage: 422
lastpage: 437
page: 422-437
order: 422
cycles: false
bibtex_author: Wang, Jun-Kun and Li, Xiaoyun and Karimi, Belhal and Li, Ping
author:
- given: Jun-Kun
  family: Wang
- given: Xiaoyun
  family: Li
- given: Belhal
  family: Karimi
- given: Ping
  family: Li
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/wang21c/wang21c.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/wang21c/wang21c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
