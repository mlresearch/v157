---
title: calibrated adversarial training
section: Contributed Papers
crossref: acml21
abstract: Adversarial training is an approach of increasing the robustness of models
  to adversarial attacks by including adversarial examples in the training set. One
  major challenge of producing adversarial examples is to contain sufficient perturbation
  in the example to flip the model’s output while not making severe changes in the
  example’s semantical content. Exuberant change in the semantical content could also
  change the true label of the example. Adding such examples to the training set results
  in adverse effects. In this paper, we present the Calibrated Adversarial Training,
  a method that reduces the adverse effects of semantic perturbations in adversarial
  training. The method produces pixel-level adaptations to the perturbations based
  on novel calibrated robust error. We provide theoretical analysis on the calibrated
  robust error and derive an upper bound for it. Our empirical results show a superior
  performance of the Calibrated Adversarial Training over a number of public datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang21a
month: 0
tex_title: calibrated adversarial training
firstpage: 626
lastpage: 641
page: 626-641
order: 626
cycles: false
bibtex_author: Huang, Tianjin and Menkovski, Vlado and Pei, Yulong and Pechenizkiy,
  Mykola
author:
- given: Tianjin
  family: Huang
- given: Vlado
  family: Menkovski
- given: Yulong
  family: Pei
- given: Mykola
  family: Pechenizkiy
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/huang21a/huang21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/huang21a/huang21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
