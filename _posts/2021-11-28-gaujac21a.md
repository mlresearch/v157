---
title: Improving Gaussian mixture latent variable model convergence with Optimal Transport
section: Contributed Papers
crossref: acml21
abstract: Generative models with both discrete and continuous latent variables are
  highly motivated by the structure of many real-world data sets. They present, however,
  subtleties in training often manifesting in the discrete latent variable not being
  leveraged. In this paper, we show why such models struggle to train using traditional
  log-likelihood maximization, and that they are amenable to training using the Optimal
  Transport framework of Wasserstein Autoencoders. We find our discrete latent variable
  to be fully leveraged by the model when trained, without any modifications to the
  objective function or significant fine tuning. Our model generates comparable samples
  to other approaches while using relatively simple neural networks, since the discrete
  latent variable carries much of the descriptive burden. Furthermore, the discrete
  latent provides significant control over generation.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gaujac21a
month: 0
tex_title: Improving Gaussian mixture latent variable model convergence with Optimal
  Transport
firstpage: 737
lastpage: 752
page: 737-752
order: 737
cycles: false
bibtex_author: Gaujac, Benoit and Feige, Ilya and Barber, David
author:
- given: Benoit
  family: Gaujac
- given: Ilya
  family: Feige
- given: David
  family: Barber
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/gaujac21a/gaujac21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
