---
title: 'Ensembling With a Fixed Parameter Budget: When Does It Help and Why?'
section: Contributed Papers
crossref: acml21
abstract: 'Given a fixed parameter budget, one can build a single large neural network
  or create a memory-split ensemble: a pool of several smaller networks with the same
  total parameter count as the single network. A memory-split ensemble can outperform
  its single model counterpart (Lobacheva et al., 2020): a phenomenon known as the
  memory-split advantage (MSA). The reasons for MSA are still not yet fully understood.
  In particular, it is difficult in practice to predict when it will exist. This paper
  sheds light on the reasons underlying MSA using random feature theory. We study
  the dependence of the MSA on several factors: the parameter budget, the training
  set size, the L2 regularization and the Stochastic Gradient Descent (SGD) hyper-parameters.
  Using the bias-variance decomposition, we show that MSA exists when the reduction
  in variance due to the ensemble (\ie, \textit{ensemble gain}) exceeds the increase
  in squared bias due to the smaller size of the individual networks (\ie, \textit{shrinkage
  cost}). Taken together, our theoretical analysis demonstrates that the MSA mainly
  exists for the small parameter budgets relative to the training set size, and that
  memory-splitting can be understood as a type of regularization. Adding other forms
  of regularization, \eg L2 regularization, reduces the MSA. Thus, the potential benefit
  of memory-splitting lies primarily in the possibility of speed-up via parallel computation.
  Our empirical experiments with deep neural networks and large image datasets show
  that MSA is not a general phenomenon, but mainly exists when the number of training
  iterations is small.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: deng21a
month: 0
tex_title: 'Ensembling With a Fixed Parameter Budget: When Does It Help and Why?'
firstpage: 1176
lastpage: 1191
page: 1176-1191
order: 1176
cycles: false
bibtex_author: Deng, Didan and Shi, Emil Bertram
author:
- given: Didan
  family: Deng
- given: Emil Bertram
  family: Shi
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/deng21a/deng21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
