---
title: Revisiting Weight Initialization of Deep Neural Networks
section: Contributed Papers
crossref: acml21
abstract: The proper {\em initialization of weights} is crucial for the effective
  training and fast convergence of {\em deep neural networks} (DNNs). Prior work in
  this area has mostly focused on the principle of {\em balancing the variance among
  weights per layer} to maintain stability of (i) the input data propagated forwards
  through the network, and (ii) the loss gradients propagated backwards, respectively.
  This prevalent heuristic is however agnostic of dependencies among gradients across
  the various layers and captures only first-order effects per layer. In this paper,
  we investigate a {\em unifying approach}, based on approximating and controlling
  the {\em norm of the layersâ€™ Hessians}, which both generalizes and explains existing
  initialization schemes such as {\em smooth activation functions}, {\em Dropouts},
  and {\em ReLU}.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: skorski21a
month: 0
tex_title: Revisiting Weight Initialization of Deep Neural Networks
firstpage: 1192
lastpage: 1207
page: 1192-1207
order: 1192
cycles: false
bibtex_author: Skorski, Maciej and Temperoni, Alessandro and Theobald, Martin
author:
- given: Maciej
  family: Skorski
- given: Alessandro
  family: Temperoni
- given: Martin
  family: Theobald
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/skorski21a/skorski21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/skorski21a/skorski21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
