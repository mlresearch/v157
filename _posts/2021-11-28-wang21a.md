---
title: 'Understanding How Over-Parametrization Leads to Acceleration: A case of learning
  a single teacher neuron'
section: Contributed Papers
crossref: acml21
abstract: Over-parametrization has become a popular technique in deep learning. It
  is observed that by over-parametrization, a larger neural network needs a fewer
  training iterations than a smaller one to achieve a certain level of performance
  â€” namely, over-parametrization leads to acceleration in optimization. However, despite
  that over-parametrization is widely used nowadays, little theory is available to
  explain the acceleration due to over-parametrization. In this paper, we propose
  understanding it by studying a simple problem first. Specifically, we consider the
  setting that there is a single teacher neuron with quadratic activation, where over-parametrization
  is realized by having multiple student neurons learn the data generated from the
  teacher neuron. We provably show that over-parametrization helps the iterate generated
  by gradient descent to enter the neighborhood of a global optimal solution that
  achieves zero testing error faster.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang21a
month: 0
tex_title: 'Understanding How Over-Parametrization Leads to Acceleration: A case of
  learning a single teacher neuron'
firstpage: 17
lastpage: 32
page: 17-32
order: 17
cycles: false
bibtex_author: Wang, Jun-Kun and Abernethy, Jacob
author:
- given: Jun-Kun
  family: Wang
- given: Jacob
  family: Abernethy
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/wang21a/wang21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/wang21a/wang21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
