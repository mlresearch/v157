---
title: Meta-Model-Based Meta-Policy Optimization
section: Contributed Papers
crossref: acml21
abstract: Model-based meta-reinforcement learning (RL) methods have recently been
  shown to be a promising approach to improving the sample efficiency of RL in multi-task
  settings. However, the theoretical understanding of those methods is yet to be established,
  and there is currently no theoretical guarantee of their performance in a real-world
  environment. In this paper, we analyze the performance guarantee of model-based
  meta-RL methods by extending the theorems proposed by Janner et al. (2019). On the
  basis of our theoretical results, we propose Meta-Model-Based Meta-Policy Optimization
  (M3PO), a model-based meta-RL method with a performance guarantee. We demonstrate
  that M3PO outperforms existing meta-RL methods in continuous-control benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hiraoka21a
month: 0
tex_title: Meta-Model-Based Meta-Policy Optimization
firstpage: 129
lastpage: 144
page: 129-144
order: 129
cycles: false
bibtex_author: Hiraoka, Takuya and Imagawa, Takahisa and Tangkaratt, Voot and Osa,
  Takayuki and Onishi, Takashi and Tsuruoka, Yoshimasa
author:
- given: Takuya
  family: Hiraoka
- given: Takahisa
  family: Imagawa
- given: Voot
  family: Tangkaratt
- given: Takayuki
  family: Osa
- given: Takashi
  family: Onishi
- given: Yoshimasa
  family: Tsuruoka
date: 2021-11-28
address:
container-title: Proceedings of The 13th Asian Conference on Machine Learning
volume: '157'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 11
  - 28
pdf: https://proceedings.mlr.press/v157/hiraoka21a/hiraoka21a.pdf
extras:
- label: Supplementary PDF
  link: https://proceedings.mlr.press/v157/hiraoka21a/hiraoka21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
